python3 > -u -m torch.distributed.launch --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 6002 .//finetune_bert_bmt.py --model-config /data/home/scv0540/.cache/model_center/bert-large-uncased/ --base-path ./ --dataset_name BoolQ --batch-size 24 --lr 0.0001 --max-decoder-length 512 --train-iters 1400 --lr-decay-style constant --weight-decay 1e-2 --loss-scale 128
/data/home/scv0540/miniconda3/envs/bmpretrain/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
====================== Initialization ============================================ Initialization ============================================ Initialization ============================================ Initialization ============================================ Initialization ============================================ Initialization ======================


====================== Initialization ============================================ Initialization ======================rank :          5
local_rank :    5
world_size :    8
local_size :    8
master :        localhost:6002
device :        5
cpus :          [42, 43, 44, 45, 46, 47, 48, 49]




rank :          7
local_rank :    7
world_size :    8
local_size :    8
master :        localhost:6002
device :        7
cpus :          [58, 59, 60, 61, 62, 63, 64, 65]
rank :          4
local_rank :    4
world_size :    8
local_size :    8
master :        localhost:6002
device :        4
cpus :          [34, 35, 36, 37, 38, 39, 40, 41]



rank :          3
local_rank :    3
world_size :    8
local_size :    8
master :        localhost:6002
device :        3
cpus :          [26, 27, 28, 29, 30, 31, 32, 33]
rank :          2
local_rank :    2
world_size :    8
local_size :    8
master :        localhost:6002
device :        2
cpus :          [18, 19, 20, 21, 22, 23, 24, 25]
rank :          6
local_rank :    6
world_size :    8
local_size :    8
master :        localhost:6002
device :        6
cpus :          [50, 51, 52, 53, 54, 55, 56, 57]


rank :          0
local_rank :    0
world_size :    8
local_size :    8
master :        localhost:6002
device :        0
cpus :          [2, 3, 4, 5, 6, 7, 8, 9]

rank :          1
local_rank :    1
world_size :    8
local_size :    8
master :        localhost:6002
device :        1
cpus :          [10, 11, 12, 13, 14, 15, 16, 17]



load from local file: /data/home/scv0540/.cache/model_center/bert-large-uncased/
load from local file: /data/home/scv0540/.cache/model_center/bert-large-uncased/
load from local file: /data/home/scv0540/.cache/model_center/bert-large-uncased/
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   89772 KB |  335532 KB |    2906 MB |    2818 MB |
|       from large pool |   15261 KB |  261021 KB |    2818 MB |    2804 MB |
|       from small pool |   74511 KB |   74750 KB |      87 MB |      14 MB |
|---------------------------------------------------------------------------|
| Active memory         |   89772 KB |  335532 KB |    2906 MB |    2818 MB |
|       from large pool |   15261 KB |  261021 KB |    2818 MB |    2804 MB |
|       from small pool |   74511 KB |   74750 KB |      87 MB |      14 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  344064 KB |  344064 KB |  344064 KB |       0 B  |
|       from large pool |  266240 KB |  266240 KB |  266240 KB |       0 B  |
|       from small pool |   77824 KB |   77824 KB |   77824 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    6483 KB |  121172 KB |   17601 MB |   17595 MB |
|       from large pool |    5219 KB |  119907 KB |   17536 MB |   17531 MB |
|       from small pool |    1264 KB |    3312 KB |      65 MB |      64 MB |
|---------------------------------------------------------------------------|
| Allocations           |     398    |     401    |    4125    |    3727    |
|       from large pool |       2    |       4    |     300    |     298    |
|       from small pool |     396    |     399    |    3825    |    3429    |
|---------------------------------------------------------------------------|
| Active allocs         |     398    |     401    |    4125    |    3727    |
|       from large pool |       2    |       4    |     300    |     298    |
|       from small pool |     396    |     399    |    3825    |    3429    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      41    |      41    |      41    |       0    |
|       from large pool |       3    |       3    |       3    |       0    |
|       from small pool |      38    |      38    |      38    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       3    |       5    |    1452    |    1449    |
|       from large pool |       1    |       2    |     147    |     146    |
|       from small pool |       2    |       3    |    1305    |    1303    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

 name                                                                   shape               max       min       std       mean      grad_std  grad_mean
 bert.input_embedding.weight                                            (30522, 1024)       1.1377    -0.5918   0.0415    -0.0194   None      None     
 bert.position_embedding.weight                                         (512, 1024)         1.0654    -0.4939   0.0166    0.0000    None      None     
 bert.token_type_embedding.weight                                       (2, 1024)           0.2937    -1.0020   0.0384    -0.0008   None      None     
 bert.encoder.layers.0.self_att.layernorm_before_attention.weight       (1024,)             0.9990    0.0861    0.1448    0.8428    None      None     
 bert.encoder.layers.0.self_att.layernorm_before_attention.bias         (1024,)             0.8135    -0.8174   0.0541    -0.0152   None      None     
 bert.encoder.layers.0.self_att.self_attention.project_q.weight         (1024, 1024)        0.4282    -0.5000   0.0352    -0.0000   None      None     
 bert.encoder.layers.0.self_att.self_attention.project_q.bias           (1024,)             0.7026    -0.6665   0.1779    -0.0031   None      None     
 bert.encoder.layers.0.self_att.self_attention.project_k.weight         (1024, 1024)        0.7183    -0.6265   0.0348    0.0000    None      None     
 bert.encoder.layers.0.self_att.self_attention.project_k.bias           (1024,)             0.0156    -0.0115   0.0024    0.0001    None      None     
 bert.encoder.layers.0.self_att.self_attention.project_v.weight         (1024, 1024)        0.1362    -0.1560   0.0261    0.0000    None      None     
 bert.encoder.layers.0.self_att.self_attention.project_v.bias           (1024,)             0.1478    -0.1490   0.0224    -0.0006   None      None     
 bert.encoder.layers.0.self_att.self_attention.attention_out.weight     (1024, 1024)        0.5752    -0.9966   0.0252    -0.0000   None      None     
 bert.encoder.layers.0.self_att.self_attention.attention_out.bias       (1024,)             0.7061    -0.5200   0.0527    -0.0035   None      None     
 bert.encoder.layers.0.ffn.layernorm_before_ffn.weight                  (1024,)             3.7988    0.8237    0.0959    1.0371    None      None     
 bert.encoder.layers.0.ffn.layernorm_before_ffn.bias                    (1024,)             3.8926    -2.1035   0.2815    -0.0074   None      None     
 bert.encoder.layers.0.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.4097    -0.3906   0.0348    0.0001    None      None     
 bert.encoder.layers.0.ffn.ffn.w_in.w.bias                              (4096,)             0.1924    -0.2117   0.0210    -0.0698   None      None     
 bert.encoder.layers.0.ffn.ffn.w_out.weight                             (1024, 4096)        2.8086    -0.6528   0.0320    -0.0000   None      None     
 bert.encoder.layers.0.ffn.ffn.w_out.bias                               (1024,)             0.1000    -0.1461   0.0267    -0.0008   None      None     
 bert.encoder.layers.1.self_att.layernorm_before_attention.weight       (1024,)             0.9507    0.2445    0.0714    0.8843    None      None     
 bert.encoder.layers.1.self_att.layernorm_before_attention.bias         (1024,)             0.6221    -0.3816   0.0715    0.0230    None      None     
 bert.encoder.layers.1.self_att.self_attention.project_q.weight         (1024, 1024)        0.3215    -0.3188   0.0317    0.0000    None      None     
 bert.encoder.layers.1.self_att.self_attention.project_q.bias           (1024,)             0.2161    -0.2981   0.0540    -0.0009   None      None     
 bert.encoder.layers.1.self_att.self_attention.project_k.weight         (1024, 1024)        0.3342    -0.4209   0.0312    0.0000    None      None     
 bert.encoder.layers.1.self_att.self_attention.project_k.bias           (1024,)             0.0108    -0.0095   0.0021    0.0000    None      None     
 bert.encoder.layers.1.self_att.self_attention.project_v.weight         (1024, 1024)        0.1467    -0.1525   0.0272    0.0000    None      None     
 bert.encoder.layers.1.self_att.self_attention.project_v.bias           (1024,)             0.0470    -0.0346   0.0115    0.0002    None      None     
 bert.encoder.layers.1.self_att.self_attention.attention_out.weight     (1024, 1024)        0.9043    -0.8926   0.0252    0.0000    None      None     
 bert.encoder.layers.1.self_att.self_attention.attention_out.bias       (1024,)             0.2002    -0.1604   0.0232    -0.0009   None      None     
 bert.encoder.layers.1.ffn.layernorm_before_ffn.weight                  (1024,)             2.9688    0.7593    0.0685    1.0215    None      None     
 bert.encoder.layers.1.ffn.layernorm_before_ffn.bias                    (1024,)             2.0820    -1.0947   0.1711    -0.0009   None      None     
 bert.encoder.layers.1.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.2423    -0.2346   0.0357    0.0001    None      None     
 bert.encoder.layers.1.ffn.ffn.w_in.w.bias                              (4096,)             0.1454    -0.1181   0.0236    -0.0535   None      None     
 bert.encoder.layers.1.ffn.ffn.w_out.weight                             (1024, 4096)        1.7178    -0.4333   0.0332    -0.0000   None      None     
 bert.encoder.layers.1.ffn.ffn.w_out.bias                               (1024,)             0.2144    -0.2249   0.0384    -0.0006   None      None     
 bert.encoder.layers.2.self_att.layernorm_before_attention.weight       (1024,)             0.9937    0.4019    0.0600    0.9263    None      None     
 bert.encoder.layers.2.self_att.layernorm_before_attention.bias         (1024,)             0.4900    -0.2166   0.0745    0.0261    None      None     
 bert.encoder.layers.2.self_att.self_attention.project_q.weight         (1024, 1024)        0.2461    -0.2527   0.0338    0.0001    None      None     
 bert.encoder.layers.2.self_att.self_attention.project_q.bias           (1024,)             0.2739    -0.2308   0.0467    -0.0007   None      None     
 bert.encoder.layers.2.self_att.self_attention.project_k.weight         (1024, 1024)        0.2489    -0.3281   0.0325    0.0000    None      None     
 bert.encoder.layers.2.self_att.self_attention.project_k.bias           (1024,)             0.0125    -0.0162   0.0020    -0.0000   None      None     
 bert.encoder.layers.2.self_att.self_attention.project_v.weight         (1024, 1024)        0.1772    -0.1851   0.0247    0.0000    None      None     
 bert.encoder.layers.2.self_att.self_attention.project_v.bias           (1024,)             0.0525    -0.0776   0.0131    -0.0002   None      None     
 bert.encoder.layers.2.self_att.self_attention.attention_out.weight     (1024, 1024)        0.4307    -0.2578   0.0234    0.0000    None      None     
 bert.encoder.layers.2.self_att.self_attention.attention_out.bias       (1024,)             0.0874    -0.1223   0.0273    -0.0010   None      None     
 bert.encoder.layers.2.ffn.layernorm_before_ffn.weight                  (1024,)             2.9043    0.8418    0.0681    0.9961    None      None     
 bert.encoder.layers.2.ffn.layernorm_before_ffn.bias                    (1024,)             2.0645    -0.9121   0.1505    0.0002    None      None     
 bert.encoder.layers.2.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.2522    -0.2308   0.0365    0.0002    None      None     
 bert.encoder.layers.2.ffn.ffn.w_in.w.bias                              (4096,)             0.1918    -0.1190   0.0255    -0.0525   None      None     
 bert.encoder.layers.2.ffn.ffn.w_out.weight                             (1024, 4096)        1.5469    -0.5684   0.0339    -0.0000   None      None     
 bert.encoder.layers.2.ffn.ffn.w_out.bias                               (1024,)             0.1497    -0.1304   0.0305    -0.0003   None      None     
 bert.encoder.layers.3.self_att.layernorm_before_attention.weight       (1024,)             0.9985    0.4033    0.0652    0.9121    None      None     
 bert.encoder.layers.3.self_att.layernorm_before_attention.bias         (1024,)             0.4116    -0.2488   0.0778    0.0249    None      None     
 bert.encoder.layers.3.self_att.self_attention.project_q.weight         (1024, 1024)        0.2500    -0.2524   0.0370    -0.0000   None      None     
 bert.encoder.layers.3.self_att.self_attention.project_q.bias           (1024,)             0.2001    -0.1951   0.0424    -0.0003   None      None     
 bert.encoder.layers.3.self_att.self_attention.project_k.weight         (1024, 1024)        0.3110    -0.2654   0.0359    0.0000    None      None     
 bert.encoder.layers.3.self_att.self_attention.project_k.bias           (1024,)             0.0176    -0.0210   0.0030    -0.0001   None      None     
 bert.encoder.layers.3.self_att.self_attention.project_v.weight         (1024, 1024)        0.1791    -0.1589   0.0254    0.0000    None      None     
 bert.encoder.layers.3.self_att.self_attention.project_v.bias           (1024,)             0.0985    -0.0781   0.0156    0.0005    None      None     
 bert.encoder.layers.3.self_att.self_attention.attention_out.weight     (1024, 1024)        0.2311    -0.1985   0.0241    0.0000    None      None     
 bert.encoder.layers.3.self_att.self_attention.attention_out.bias       (1024,)             0.1602    -0.1678   0.0454    -0.0012   None      None     
 bert.encoder.layers.3.ffn.layernorm_before_ffn.weight                  (1024,)             2.6074    0.7925    0.0635    0.9805    None      None     
 bert.encoder.layers.3.ffn.layernorm_before_ffn.bias                    (1024,)             2.2051    -0.6562   0.1462    0.0036    None      None     
 bert.encoder.layers.3.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.2134    -0.2046   0.0370    0.0002    None      None     
 bert.encoder.layers.3.ffn.ffn.w_in.w.bias                              (4096,)             0.2195    -0.1488   0.0256    -0.0529   None      None     
 bert.encoder.layers.3.ffn.ffn.w_out.weight                             (1024, 4096)        0.6123    -0.4709   0.0341    -0.0000   None      None     
 bert.encoder.layers.3.ffn.ffn.w_out.bias                               (1024,)             0.2242    -0.0938   0.0323    0.0003    None      None     
 bert.encoder.layers.4.self_att.layernorm_before_attention.weight       (1024,)             0.9873    0.3335    0.0537    0.9062    None      None     
 bert.encoder.layers.4.self_att.layernorm_before_attention.bias         (1024,)             0.2954    -0.1881   0.0698    0.0255    None      None     
 bert.encoder.layers.4.self_att.self_attention.project_q.weight         (1024, 1024)        0.2646    -0.3071   0.0384    -0.0000   None      None     
 bert.encoder.layers.4.self_att.self_attention.project_q.bias           (1024,)             0.2092    -0.2375   0.0411    0.0002    None      None     
 bert.encoder.layers.4.self_att.self_attention.project_k.weight         (1024, 1024)        0.2314    -0.3259   0.0376    0.0000    None      None     
 bert.encoder.layers.4.self_att.self_attention.project_k.bias           (1024,)             0.0188    -0.0131   0.0030    0.0000    None      None     
 bert.encoder.layers.4.self_att.self_attention.project_v.weight         (1024, 1024)        0.1356    -0.1418   0.0252    -0.0000   None      None     
 bert.encoder.layers.4.self_att.self_attention.project_v.bias           (1024,)             0.2152    -0.1823   0.0198    -0.0002   None      None     
 bert.encoder.layers.4.self_att.self_attention.attention_out.weight     (1024, 1024)        0.2184    -0.1849   0.0238    0.0000    None      None     
 bert.encoder.layers.4.self_att.self_attention.attention_out.bias       (1024,)             0.1123    -0.0791   0.0178    -0.0008   None      None     
 bert.encoder.layers.4.ffn.layernorm_before_ffn.weight                  (1024,)             2.7383    0.7598    0.0805    0.9619    None      None     
 bert.encoder.layers.4.ffn.layernorm_before_ffn.bias                    (1024,)             1.7139    -0.5405   0.1422    0.0034    None      None     
 bert.encoder.layers.4.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.3591    -0.3303   0.0369    0.0002    None      None     
 bert.encoder.layers.4.ffn.ffn.w_in.w.bias                              (4096,)             0.2180    -0.1722   0.0265    -0.0539   None      None     
 bert.encoder.layers.4.ffn.ffn.w_out.weight                             (1024, 4096)        3.2383    -0.5815   0.0344    0.0000    None      None     
 bert.encoder.layers.4.ffn.ffn.w_out.bias                               (1024,)             0.1664    -0.1884   0.0370    0.0003    None      None     
 bert.encoder.layers.5.self_att.layernorm_before_attention.weight       (1024,)             0.9873    0.1809    0.0573    0.9053    None      None     
 bert.encoder.layers.5.self_att.layernorm_before_attention.bias         (1024,)             0.4263    -0.4619   0.0619    0.0320    None      None     
 bert.encoder.layers.5.self_att.self_attention.project_q.weight         (1024, 1024)        0.2449    -0.3179   0.0368    -0.0000   None      None     
 bert.encoder.layers.5.self_att.self_attention.project_q.bias           (1024,)             0.2086    -0.2881   0.0431    0.0021    None      None     
 bert.encoder.layers.5.self_att.self_attention.project_k.weight         (1024, 1024)        0.3562    -0.3430   0.0361    -0.0001   None      None     
 bert.encoder.layers.5.self_att.self_attention.project_k.bias           (1024,)             0.0155    -0.0194   0.0027    0.0000    None      None     
 bert.encoder.layers.5.self_att.self_attention.project_v.weight         (1024, 1024)        0.1929    -0.1580   0.0277    0.0000    None      None     
 bert.encoder.layers.5.self_att.self_attention.project_v.bias           (1024,)             0.0337    -0.0388   0.0110    0.0005    None      None     
 bert.encoder.layers.5.self_att.self_attention.attention_out.weight     (1024, 1024)        0.2484    -0.1782   0.0259    0.0000    None      None     
 bert.encoder.layers.5.self_att.self_attention.attention_out.bias       (1024,)             0.2678    -0.0789   0.0180    -0.0002   None      None     
 bert.encoder.layers.5.ffn.layernorm_before_ffn.weight                  (1024,)             3.0332    0.7607    0.0932    0.9438    None      None     
 bert.encoder.layers.5.ffn.layernorm_before_ffn.bias                    (1024,)             1.9629    -0.9380   0.1594    0.0058    None      None     
 bert.encoder.layers.5.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.2612    -0.2489   0.0370    0.0001    None      None     
 bert.encoder.layers.5.ffn.ffn.w_in.w.bias                              (4096,)             0.2793    -0.1738   0.0261    -0.0557   None      None     
 bert.encoder.layers.5.ffn.ffn.w_out.weight                             (1024, 4096)        3.1328    -0.5703   0.0349    0.0000    None      None     
 bert.encoder.layers.5.ffn.ffn.w_out.bias                               (1024,)             0.2101    -0.1603   0.0384    0.0003    None      None     
 bert.encoder.layers.6.self_att.layernorm_before_attention.weight       (1024,)             0.9575    0.1455    0.0546    0.8657    None      None     
 bert.encoder.layers.6.self_att.layernorm_before_attention.bias         (1024,)             0.8320    -1.0928   0.0767    0.0306    None      None     
 bert.encoder.layers.6.self_att.self_attention.project_q.weight         (1024, 1024)        0.3110    -0.3477   0.0366    0.0000    None      None     
 bert.encoder.layers.6.self_att.self_attention.project_q.bias           (1024,)             0.3567    -0.3845   0.0589    0.0005    None      None     
 bert.encoder.layers.6.self_att.self_attention.project_k.weight         (1024, 1024)        0.5234    -0.5889   0.0358    0.0000    None      None     
 bert.encoder.layers.6.self_att.self_attention.project_k.bias           (1024,)             0.0116    -0.0121   0.0029    0.0000    None      None     
 bert.encoder.layers.6.self_att.self_attention.project_v.weight         (1024, 1024)        0.1423    -0.1426   0.0276    0.0000    None      None     
 bert.encoder.layers.6.self_att.self_attention.project_v.bias           (1024,)             0.0395    -0.0360   0.0101    -0.0001   None      None     
 bert.encoder.layers.6.self_att.self_attention.attention_out.weight     (1024, 1024)        0.2202    -0.2896   0.0266    -0.0000   None      None     
 bert.encoder.layers.6.self_att.self_attention.attention_out.bias       (1024,)             0.4751    -0.1036   0.0309    -0.0004   None      None     
 bert.encoder.layers.6.ffn.layernorm_before_ffn.weight                  (1024,)             3.2891    0.8120    0.0891    0.9521    None      None     
 bert.encoder.layers.6.ffn.layernorm_before_ffn.bias                    (1024,)             2.3711    -1.4141   0.1614    0.0041    None      None     
 bert.encoder.layers.6.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.2856    -0.2358   0.0371    0.0000    None      None     
 bert.encoder.layers.6.ffn.ffn.w_in.w.bias                              (4096,)             0.2661    -0.1384   0.0289    -0.0570   None      None     
 bert.encoder.layers.6.ffn.ffn.w_out.weight                             (1024, 4096)        3.6094    -0.7188   0.0353    -0.0000   None      None     
 bert.encoder.layers.6.ffn.ffn.w_out.bias                               (1024,)             0.5161    -0.1433   0.0433    0.0007    None      None     
 bert.encoder.layers.7.self_att.layernorm_before_attention.weight       (1024,)             0.9453    0.1019    0.0511    0.8560    None      None     
 bert.encoder.layers.7.self_att.layernorm_before_attention.bias         (1024,)             0.2576    -1.2031   0.0755    0.0195    None      None     
 bert.encoder.layers.7.self_att.self_attention.project_q.weight         (1024, 1024)        0.3240    -0.3511   0.0356    -0.0000   None      None     
 bert.encoder.layers.7.self_att.self_attention.project_q.bias           (1024,)             0.2771    -0.3000   0.0745    0.0002    None      None     
 bert.encoder.layers.7.self_att.self_attention.project_k.weight         (1024, 1024)        0.3784    -0.3582   0.0351    0.0000    None      None     
 bert.encoder.layers.7.self_att.self_attention.project_k.bias           (1024,)             0.0107    -0.0106   0.0025    0.0001    None      None     
 bert.encoder.layers.7.self_att.self_attention.project_v.weight         (1024, 1024)        0.1388    -0.1392   0.0279    -0.0000   None      None     
 bert.encoder.layers.7.self_att.self_attention.project_v.bias           (1024,)             0.0433    -0.0413   0.0133    0.0003    None      None     
 bert.encoder.layers.7.self_att.self_attention.attention_out.weight     (1024, 1024)        0.2100    -0.2554   0.0270    -0.0000   None      None     
 bert.encoder.layers.7.self_att.self_attention.attention_out.bias       (1024,)             0.2896    -0.1913   0.0213    0.0005    None      None     
 bert.encoder.layers.7.ffn.layernorm_before_ffn.weight                  (1024,)             2.5195    0.8247    0.0685    0.9614    None      None     
 bert.encoder.layers.7.ffn.layernorm_before_ffn.bias                    (1024,)             1.2031    -1.7314   0.1417    -0.0020   None      None     
 bert.encoder.layers.7.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.2542    -0.2571   0.0369    -0.0000   None      None     
 bert.encoder.layers.7.ffn.ffn.w_in.w.bias                              (4096,)             0.3118    -0.1895   0.0328    -0.0558   None      None     
 bert.encoder.layers.7.ffn.ffn.w_out.weight                             (1024, 4096)        2.8340    -1.4365   0.0352    -0.0001   None      None     
 bert.encoder.layers.7.ffn.ffn.w_out.bias                               (1024,)             0.5825    -0.1724   0.0438    0.0005    None      None     
 bert.encoder.layers.8.self_att.layernorm_before_attention.weight       (1024,)             0.9648    0.0859    0.0532    0.8774    None      None     
 bert.encoder.layers.8.self_att.layernorm_before_attention.bias         (1024,)             0.2338    -1.2373   0.0806    -0.0064   None      None     
 bert.encoder.layers.8.self_att.self_attention.project_q.weight         (1024, 1024)        0.4102    -0.4207   0.0361    0.0000    None      None     
 bert.encoder.layers.8.self_att.self_attention.project_q.bias           (1024,)             0.5029    -0.4517   0.0989    -0.0008   None      None     
 bert.encoder.layers.8.self_att.self_attention.project_k.weight         (1024, 1024)        0.4861    -0.3933   0.0356    -0.0000   None      None     
 bert.encoder.layers.8.self_att.self_attention.project_k.bias           (1024,)             0.0117    -0.0131   0.0027    -0.0000   None      None     
 bert.encoder.layers.8.self_att.self_attention.project_v.weight         (1024, 1024)        0.1351    -0.1520   0.0270    0.0000    None      None     
 bert.encoder.layers.8.self_att.self_attention.project_v.bias           (1024,)             0.0579    -0.0533   0.0150    -0.0006   None      None     
 bert.encoder.layers.8.self_att.self_attention.attention_out.weight     (1024, 1024)        0.3484    -0.5840   0.0259    -0.0000   None      None     
 bert.encoder.layers.8.self_att.self_attention.attention_out.bias       (1024,)             0.1179    -0.1196   0.0175    0.0001    None      None     
 bert.encoder.layers.8.ffn.layernorm_before_ffn.weight                  (1024,)             2.5254    0.6904    0.0695    0.9521    None      None     
 bert.encoder.layers.8.ffn.layernorm_before_ffn.bias                    (1024,)             0.7578    -1.7656   0.1354    -0.0084   None      None     
 bert.encoder.layers.8.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.2783    -0.2737   0.0363    -0.0001   None      None     
 bert.encoder.layers.8.ffn.ffn.w_in.w.bias                              (4096,)             0.2898    -0.1740   0.0394    -0.0554   None      None     
 bert.encoder.layers.8.ffn.ffn.w_out.weight                             (1024, 4096)        2.3398    -1.2324   0.0343    -0.0000   None      None     
 bert.encoder.layers.8.ffn.ffn.w_out.bias                               (1024,)             0.1161    -0.3237   0.0376    -0.0002   None      None     
 bert.encoder.layers.9.self_att.layernorm_before_attention.weight       (1024,)             1.0342    0.4377    0.0695    0.9111    None      None     
 bert.encoder.layers.9.self_att.layernorm_before_attention.bias         (1024,)             0.2095    -0.5908   0.0637    -0.0229   None      None     
 bert.encoder.layers.9.self_att.self_attention.project_q.weight         (1024, 1024)        0.2849    -0.2747   0.0381    0.0001    None      None     
 bert.encoder.layers.9.self_att.self_attention.project_q.bias           (1024,)             0.4883    -0.3865   0.0812    -0.0036   None      None     
 bert.encoder.layers.9.self_att.self_attention.project_k.weight         (1024, 1024)        0.3247    -0.2510   0.0372    -0.0000   None      None     
 bert.encoder.layers.9.self_att.self_attention.project_k.bias           (1024,)             0.0181    -0.0171   0.0033    -0.0000   None      None     
 bert.encoder.layers.9.self_att.self_attention.project_v.weight         (1024, 1024)        0.1992    -0.2542   0.0255    -0.0000   None      None     
 bert.encoder.layers.9.self_att.self_attention.project_v.bias           (1024,)             0.1068    -0.1820   0.0219    -0.0001   None      None     
 bert.encoder.layers.9.self_att.self_attention.attention_out.weight     (1024, 1024)        0.4885    -0.2756   0.0241    -0.0000   None      None     
 bert.encoder.layers.9.self_att.self_attention.attention_out.bias       (1024,)             0.2544    -0.1865   0.0572    -0.0001   None      None     
 bert.encoder.layers.9.ffn.layernorm_before_ffn.weight                  (1024,)             2.3477    0.7695    0.0741    0.9453    None      None     
 bert.encoder.layers.9.ffn.layernorm_before_ffn.bias                    (1024,)             0.7085    -1.7334   0.1221    -0.0106   None      None     
 bert.encoder.layers.9.ffn.ffn.w_in.w.weight                            (4096, 1024)        0.3340    -0.3318   0.0360    -0.0001   None      None     
 bert.encoder.layers.9.ffn.ffn.w_in.w.bias                              (4096,)             0.2737    -0.1656   0.0371    -0.0544   None      None     
 bert.encoder.layers.9.ffn.ffn.w_out.weight                             (1024, 4096)        2.7480    -0.8911   0.0333    -0.0000   None      None     
 bert.encoder.layers.9.ffn.ffn.w_out.bias                               (1024,)             0.1108    -0.5049   0.0380    -0.0003   None      None     
 bert.encoder.layers.10.self_att.layernorm_before_attention.weight      (1024,)             1.0400    0.4946    0.0737    0.9067    None      None     
 bert.encoder.layers.10.self_att.layernorm_before_attention.bias        (1024,)             0.1761    -0.4263   0.0597    -0.0199   None      None     
 bert.encoder.layers.10.self_att.self_attention.project_q.weight        (1024, 1024)        0.2786    -0.2756   0.0381    0.0000    None      None     
 bert.encoder.layers.10.self_att.self_attention.project_q.bias          (1024,)             0.3215    -0.3159   0.0620    -0.0033   None      None     
 bert.encoder.layers.10.self_att.self_attention.project_k.weight        (1024, 1024)        0.2952    -0.2947   0.0374    0.0000    None      None     
 bert.encoder.layers.10.self_att.self_attention.project_k.bias          (1024,)             0.0171    -0.0193   0.0038    0.0000    None      None     
 bert.encoder.layers.10.self_att.self_attention.project_v.weight        (1024, 1024)        0.1465    -0.1611   0.0260    0.0000    None      None     
 bert.encoder.layers.10.self_att.self_attention.project_v.bias          (1024,)             0.1372    -0.1333   0.0253    -0.0007   None      None     
 bert.encoder.layers.10.self_att.self_attention.attention_out.weight    (1024, 1024)        0.4067    -0.3848   0.0245    0.0000    None      None     
 bert.encoder.layers.10.self_att.self_attention.attention_out.bias      (1024,)             0.1970    -0.2180   0.0585    -0.0003   None      None     
 bert.encoder.layers.10.ffn.layernorm_before_ffn.weight                 (1024,)             2.5742    0.7080    0.0848    0.9194    None      None     
 bert.encoder.layers.10.ffn.layernorm_before_ffn.bias                   (1024,)             0.8862    -1.4688   0.1243    -0.0110   None      None     
 bert.encoder.layers.10.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.3784    -0.4551   0.0358    -0.0000   None      None     
 bert.encoder.layers.10.ffn.ffn.w_in.w.bias                             (4096,)             0.2832    -0.2228   0.0326    -0.0577   None      None     
 bert.encoder.layers.10.ffn.ffn.w_out.weight                            (1024, 4096)        4.2969    -1.0928   0.0331    0.0000    None      None     
 bert.encoder.layers.10.ffn.ffn.w_out.bias                              (1024,)             0.2047    -0.7417   0.0587    0.0001    None      None     
 bert.encoder.layers.11.self_att.layernorm_before_attention.weight      (1024,)             1.0176    0.4404    0.0630    0.8950    None      None     
 bert.encoder.layers.11.self_att.layernorm_before_attention.bias        (1024,)             0.1593    -0.5566   0.0462    -0.0009   None      None     
 bert.encoder.layers.11.self_att.self_attention.project_q.weight        (1024, 1024)        0.2654    -0.2428   0.0390    -0.0001   None      None     
 bert.encoder.layers.11.self_att.self_attention.project_q.bias          (1024,)             0.3716    -0.3713   0.0715    -0.0012   None      None     
 bert.encoder.layers.11.self_att.self_attention.project_k.weight        (1024, 1024)        0.3523    -0.3364   0.0383    0.0001    None      None     
 bert.encoder.layers.11.self_att.self_attention.project_k.bias          (1024,)             0.0240    -0.0223   0.0045    -0.0001   None      None     
 bert.encoder.layers.11.self_att.self_attention.project_v.weight        (1024, 1024)        0.1544    -0.1866   0.0290    0.0000    None      None     
 bert.encoder.layers.11.self_att.self_attention.project_v.bias          (1024,)             0.1532    -0.1431   0.0224    0.0005    None      None     
 bert.encoder.layers.11.self_att.self_attention.attention_out.weight    (1024, 1024)        0.3848    -0.3362   0.0274    -0.0000   None      None     
 bert.encoder.layers.11.self_att.self_attention.attention_out.bias      (1024,)             0.2289    -0.2048   0.0560    0.0001    None      None     
 bert.encoder.layers.11.ffn.layernorm_before_ffn.weight                 (1024,)             2.7148    0.7041    0.0889    0.9141    None      None     
 bert.encoder.layers.11.ffn.layernorm_before_ffn.bias                   (1024,)             1.0566    -1.5078   0.1188    -0.0113   None      None     
 bert.encoder.layers.11.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.5327    -0.5410   0.0363    -0.0000   None      None     
 bert.encoder.layers.11.ffn.ffn.w_in.w.bias                             (4096,)             0.2644    -0.2078   0.0313    -0.0564   None      None     
 bert.encoder.layers.11.ffn.ffn.w_out.weight                            (1024, 4096)        5.5234    -1.1250   0.0338    0.0000    None      None     
 bert.encoder.layers.11.ffn.ffn.w_out.bias                              (1024,)             0.3052    -0.7339   0.0496    0.0003    None      None     
 bert.encoder.layers.12.self_att.layernorm_before_attention.weight      (1024,)             0.9985    0.3870    0.0620    0.8823    None      None     
 bert.encoder.layers.12.self_att.layernorm_before_attention.bias        (1024,)             0.1605    -0.4900   0.0485    0.0017    None      None     
 bert.encoder.layers.12.self_att.self_attention.project_q.weight        (1024, 1024)        0.2568    -0.2556   0.0383    0.0000    None      None     
 bert.encoder.layers.12.self_att.self_attention.project_q.bias          (1024,)             0.4014    -0.3088   0.0652    -0.0029   None      None     
 bert.encoder.layers.12.self_att.self_attention.project_k.weight        (1024, 1024)        0.3630    -0.2913   0.0380    0.0000    None      None     
 bert.encoder.layers.12.self_att.self_attention.project_k.bias          (1024,)             0.0144    -0.0117   0.0038    0.0001    None      None     
 bert.encoder.layers.12.self_att.self_attention.project_v.weight        (1024, 1024)        0.1857    -0.1675   0.0294    -0.0000   None      None     
 bert.encoder.layers.12.self_att.self_attention.project_v.bias          (1024,)             0.1053    -0.1272   0.0218    0.0005    None      None     
 bert.encoder.layers.12.self_att.self_attention.attention_out.weight    (1024, 1024)        0.2578    -0.2893   0.0282    -0.0000   None      None     
 bert.encoder.layers.12.self_att.self_attention.attention_out.bias      (1024,)             0.3625    -0.1533   0.0366    0.0002    None      None     
 bert.encoder.layers.12.ffn.layernorm_before_ffn.weight                 (1024,)             2.5723    0.7080    0.0895    0.9072    None      None     
 bert.encoder.layers.12.ffn.layernorm_before_ffn.bias                   (1024,)             1.0986    -1.5625   0.1187    -0.0126   None      None     
 bert.encoder.layers.12.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.2323    -0.4839   0.0361    -0.0000   None      None     
 bert.encoder.layers.12.ffn.ffn.w_in.w.bias                             (4096,)             0.3052    -0.2369   0.0334    -0.0590   None      None     
 bert.encoder.layers.12.ffn.ffn.w_out.weight                            (1024, 4096)        4.9805    -1.1494   0.0337    0.0000    None      None     
 bert.encoder.layers.12.ffn.ffn.w_out.bias                              (1024,)             0.4062    -0.7241   0.0580    0.0008    None      None     
 bert.encoder.layers.13.self_att.layernorm_before_attention.weight      (1024,)             0.9658    0.4333    0.0527    0.8574    None      None     
 bert.encoder.layers.13.self_att.layernorm_before_attention.bias        (1024,)             0.1530    -0.3723   0.0496    -0.0020   None      None     
 bert.encoder.layers.13.self_att.self_attention.project_q.weight        (1024, 1024)        0.2051    -0.2622   0.0383    0.0000    None      None     
 bert.encoder.layers.13.self_att.self_attention.project_q.bias          (1024,)             0.3049    -0.3469   0.0690    -0.0019   None      None     
 bert.encoder.layers.13.self_att.self_attention.project_k.weight        (1024, 1024)        0.2996    -0.3184   0.0384    0.0000    None      None     
 bert.encoder.layers.13.self_att.self_attention.project_k.bias          (1024,)             0.0156    -0.0188   0.0052    -0.0001   None      None     
 bert.encoder.layers.13.self_att.self_attention.project_v.weight        (1024, 1024)        0.1870    -0.1523   0.0289    -0.0000   None      None     
 bert.encoder.layers.13.self_att.self_attention.project_v.bias          (1024,)             0.0958    -0.1588   0.0245    -0.0007   None      None     
 bert.encoder.layers.13.self_att.self_attention.attention_out.weight    (1024, 1024)        0.2062    -0.2788   0.0284    -0.0000   None      None     
 bert.encoder.layers.13.self_att.self_attention.attention_out.bias      (1024,)             0.4028    -0.1042   0.0301    -0.0001   None      None     
 bert.encoder.layers.13.ffn.layernorm_before_ffn.weight                 (1024,)             2.6387    0.6992    0.0934    0.8833    None      None     
 bert.encoder.layers.13.ffn.layernorm_before_ffn.bias                   (1024,)             1.1416    -1.4961   0.1349    -0.0131   None      None     
 bert.encoder.layers.13.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.2957    -0.3069   0.0351    0.0000    None      None     
 bert.encoder.layers.13.ffn.ffn.w_in.w.bias                             (4096,)             0.3281    -0.2338   0.0346    -0.0614   None      None     
 bert.encoder.layers.13.ffn.ffn.w_out.weight                            (1024, 4096)        4.6172    -0.9941   0.0337    0.0000    None      None     
 bert.encoder.layers.13.ffn.ffn.w_out.bias                              (1024,)             0.3748    -0.7881   0.0695    0.0008    None      None     
 bert.encoder.layers.14.self_att.layernorm_before_attention.weight      (1024,)             0.9175    0.4087    0.0399    0.8340    None      None     
 bert.encoder.layers.14.self_att.layernorm_before_attention.bias        (1024,)             0.1467    -1.3057   0.0591    0.0025    None      None     
 bert.encoder.layers.14.self_att.self_attention.project_q.weight        (1024, 1024)        0.2766    -0.2932   0.0376    -0.0000   None      None     
 bert.encoder.layers.14.self_att.self_attention.project_q.bias          (1024,)             0.3496    -0.4731   0.0900    -0.0014   None      None     
 bert.encoder.layers.14.self_att.self_attention.project_k.weight        (1024, 1024)        0.3103    -0.3721   0.0374    -0.0000   None      None     
 bert.encoder.layers.14.self_att.self_attention.project_k.bias          (1024,)             0.0195    -0.0215   0.0057    0.0002    None      None     
 bert.encoder.layers.14.self_att.self_attention.project_v.weight        (1024, 1024)        0.1613    -0.1725   0.0301    -0.0000   None      None     
 bert.encoder.layers.14.self_att.self_attention.project_v.bias          (1024,)             0.1516    -0.1114   0.0228    0.0006    None      None     
 bert.encoder.layers.14.self_att.self_attention.attention_out.weight    (1024, 1024)        0.2729    -0.2090   0.0288    0.0000    None      None     
 bert.encoder.layers.14.self_att.self_attention.attention_out.bias      (1024,)             0.1908    -0.0894   0.0253    -0.0002   None      None     
 bert.encoder.layers.14.ffn.layernorm_before_ffn.weight                 (1024,)             2.4648    0.7349    0.0825    0.8682    None      None     
 bert.encoder.layers.14.ffn.layernorm_before_ffn.bias                   (1024,)             1.0186    -2.0664   0.1376    -0.0124   None      None     
 bert.encoder.layers.14.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.2429    -0.2214   0.0351    0.0000    None      None     
 bert.encoder.layers.14.ffn.ffn.w_in.w.bias                             (4096,)             0.3491    -0.2225   0.0320    -0.0602   None      None     
 bert.encoder.layers.14.ffn.ffn.w_out.weight                            (1024, 4096)        3.1562    -1.7764   0.0337    -0.0000   None      None     
 bert.encoder.layers.14.ffn.ffn.w_out.bias                              (1024,)             0.2542    -0.5767   0.0617    0.0005    None      None     
 bert.encoder.layers.15.self_att.layernorm_before_attention.weight      (1024,)             0.9102    0.1075    0.0419    0.8096    None      None     
 bert.encoder.layers.15.self_att.layernorm_before_attention.bias        (1024,)             1.2520    -1.8994   0.0864    -0.0050   None      None     
 bert.encoder.layers.15.self_att.self_attention.project_q.weight        (1024, 1024)        0.2798    -0.3154   0.0369    0.0000    None      None     
 bert.encoder.layers.15.self_att.self_attention.project_q.bias          (1024,)             0.4998    -0.3689   0.0999    -0.0033   None      None     
 bert.encoder.layers.15.self_att.self_attention.project_k.weight        (1024, 1024)        0.3516    -0.3354   0.0367    0.0000    None      None     
 bert.encoder.layers.15.self_att.self_attention.project_k.bias          (1024,)             0.0172    -0.0141   0.0047    -0.0000   None      None     
 bert.encoder.layers.15.self_att.self_attention.project_v.weight        (1024, 1024)        0.1483    -0.1487   0.0295    -0.0000   None      None     
 bert.encoder.layers.15.self_att.self_attention.project_v.bias          (1024,)             0.1810    -0.2390   0.0290    0.0012    None      None     
 bert.encoder.layers.15.self_att.self_attention.attention_out.weight    (1024, 1024)        0.3096    -0.5386   0.0282    -0.0000   None      None     
 bert.encoder.layers.15.self_att.self_attention.attention_out.bias      (1024,)             0.0979    -0.0941   0.0264    -0.0001   None      None     
 bert.encoder.layers.15.ffn.layernorm_before_ffn.weight                 (1024,)             2.1562    0.6890    0.0717    0.8647    None      None     
 bert.encoder.layers.15.ffn.layernorm_before_ffn.bias                   (1024,)             0.7314    -2.1250   0.1289    -0.0088   None      None     
 bert.encoder.layers.15.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.2363    -0.2146   0.0347    -0.0000   None      None     
 bert.encoder.layers.15.ffn.ffn.w_in.w.bias                             (4096,)             0.4114    -0.2673   0.0334    -0.0595   None      None     
 bert.encoder.layers.15.ffn.ffn.w_out.weight                            (1024, 4096)        1.6328    -2.6484   0.0331    -0.0000   None      None     
 bert.encoder.layers.15.ffn.ffn.w_out.bias                              (1024,)             0.2089    -0.8496   0.0610    0.0005    None      None     
 bert.encoder.layers.16.self_att.layernorm_before_attention.weight      (1024,)             0.9189    0.3330    0.0331    0.8198    None      None     
 bert.encoder.layers.16.self_att.layernorm_before_attention.bias        (1024,)             0.1940    -0.7305   0.0641    -0.0153   None      None     
 bert.encoder.layers.16.self_att.self_attention.project_q.weight        (1024, 1024)        0.2340    -0.2422   0.0378    -0.0000   None      None     
 bert.encoder.layers.16.self_att.self_attention.project_q.bias          (1024,)             0.4546    -0.4666   0.0991    0.0012    None      None     
 bert.encoder.layers.16.self_att.self_attention.project_k.weight        (1024, 1024)        0.3113    -0.3262   0.0379    0.0000    None      None     
 bert.encoder.layers.16.self_att.self_attention.project_k.bias          (1024,)             0.0194    -0.0211   0.0051    0.0002    None      None     
 bert.encoder.layers.16.self_att.self_attention.project_v.weight        (1024, 1024)        0.1427    -0.1593   0.0290    0.0000    None      None     
 bert.encoder.layers.16.self_att.self_attention.project_v.bias          (1024,)             0.0735    -0.1146   0.0227    0.0002    None      None     
 bert.encoder.layers.16.self_att.self_attention.attention_out.weight    (1024, 1024)        0.3496    -0.2491   0.0279    0.0000    None      None     
 bert.encoder.layers.16.self_att.self_attention.attention_out.bias      (1024,)             0.1288    -0.1002   0.0248    -0.0002   None      None     
 bert.encoder.layers.16.ffn.layernorm_before_ffn.weight                 (1024,)             2.1738    0.7104    0.0632    0.8696    None      None     
 bert.encoder.layers.16.ffn.layernorm_before_ffn.bias                   (1024,)             0.6069    -1.8311   0.1238    -0.0128   None      None     
 bert.encoder.layers.16.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.2262    -0.2141   0.0342    0.0000    None      None     
 bert.encoder.layers.16.ffn.ffn.w_in.w.bias                             (4096,)             0.3940    -0.2808   0.0311    -0.0596   None      None     
 bert.encoder.layers.16.ffn.ffn.w_out.weight                            (1024, 4096)        0.8706    -1.2607   0.0327    0.0000    None      None     
 bert.encoder.layers.16.ffn.ffn.w_out.bias                              (1024,)             0.2443    -0.8892   0.0587    0.0003    None      None     
 bert.encoder.layers.17.self_att.layernorm_before_attention.weight      (1024,)             0.9609    0.4055    0.0303    0.8628    None      None     
 bert.encoder.layers.17.self_att.layernorm_before_attention.bias        (1024,)             0.1514    -0.4424   0.0568    -0.0229   None      None     
 bert.encoder.layers.17.self_att.self_attention.project_q.weight        (1024, 1024)        0.2106    -0.2191   0.0405    0.0000    None      None     
 bert.encoder.layers.17.self_att.self_attention.project_q.bias          (1024,)             0.3567    -0.3606   0.0864    -0.0014   None      None     
 bert.encoder.layers.17.self_att.self_attention.project_k.weight        (1024, 1024)        0.3550    -0.3479   0.0405    -0.0000   None      None     
 bert.encoder.layers.17.self_att.self_attention.project_k.bias          (1024,)             0.0216    -0.0217   0.0059    -0.0002   None      None     
 bert.encoder.layers.17.self_att.self_attention.project_v.weight        (1024, 1024)        0.1486    -0.1750   0.0275    -0.0000   None      None     
 bert.encoder.layers.17.self_att.self_attention.project_v.bias          (1024,)             0.1359    -0.1893   0.0261    0.0012    None      None     
 bert.encoder.layers.17.self_att.self_attention.attention_out.weight    (1024, 1024)        0.1521    -0.1531   0.0267    -0.0000   None      None     
 bert.encoder.layers.17.self_att.self_attention.attention_out.bias      (1024,)             0.1278    -0.1093   0.0356    0.0001    None      None     
 bert.encoder.layers.17.ffn.layernorm_before_ffn.weight                 (1024,)             1.9502    0.7476    0.0574    0.8604    None      None     
 bert.encoder.layers.17.ffn.layernorm_before_ffn.bias                   (1024,)             0.7690    -1.7705   0.1251    -0.0122   None      None     
 bert.encoder.layers.17.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.5220    -0.5688   0.0332    0.0001    None      None     
 bert.encoder.layers.17.ffn.ffn.w_in.w.bias                             (4096,)             0.3108    -0.2129   0.0266    -0.0624   None      None     
 bert.encoder.layers.17.ffn.ffn.w_out.weight                            (1024, 4096)        1.5537    -4.4609   0.0320    -0.0000   None      None     
 bert.encoder.layers.17.ffn.ffn.w_out.bias                              (1024,)             0.3464    -0.6504   0.0591    0.0002    None      None     
 bert.encoder.layers.18.self_att.layernorm_before_attention.weight      (1024,)             0.9468    0.2556    0.0372    0.8521    None      None     
 bert.encoder.layers.18.self_att.layernorm_before_attention.bias        (1024,)             0.7393    -0.3669   0.0533    -0.0262   None      None     
 bert.encoder.layers.18.self_att.self_attention.project_q.weight        (1024, 1024)        0.2844    -0.2615   0.0390    0.0000    None      None     
 bert.encoder.layers.18.self_att.self_attention.project_q.bias          (1024,)             0.4673    -0.3740   0.0851    0.0012    None      None     
 bert.encoder.layers.18.self_att.self_attention.project_k.weight        (1024, 1024)        0.2491    -0.3225   0.0392    0.0000    None      None     
 bert.encoder.layers.18.self_att.self_attention.project_k.bias          (1024,)             0.0168    -0.0220   0.0051    0.0002    None      None     
 bert.encoder.layers.18.self_att.self_attention.project_v.weight        (1024, 1024)        0.1726    -0.1904   0.0290    0.0001    None      None     
 bert.encoder.layers.18.self_att.self_attention.project_v.bias          (1024,)             0.1836    -0.1456   0.0241    -0.0005   None      None     
 bert.encoder.layers.18.self_att.self_attention.attention_out.weight    (1024, 1024)        0.1490    -0.1361   0.0271    -0.0000   None      None     
 bert.encoder.layers.18.self_att.self_attention.attention_out.bias      (1024,)             0.1562    -0.0951   0.0316    0.0002    None      None     
 bert.encoder.layers.18.ffn.layernorm_before_ffn.weight                 (1024,)             2.0332    0.7495    0.0721    0.8384    None      None     
 bert.encoder.layers.18.ffn.layernorm_before_ffn.bias                   (1024,)             1.2002    -1.7871   0.1193    -0.0093   None      None     
 bert.encoder.layers.18.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.9453    -1.1523   0.0328    0.0001    None      None     
 bert.encoder.layers.18.ffn.ffn.w_in.w.bias                             (4096,)             0.3604    -0.2194   0.0239    -0.0660   None      None     
 bert.encoder.layers.18.ffn.ffn.w_out.weight                            (1024, 4096)        1.3574    -5.4492   0.0323    -0.0000   None      None     
 bert.encoder.layers.18.ffn.ffn.w_out.bias                              (1024,)             0.2700    -0.1882   0.0563    -0.0000   None      None     
 bert.encoder.layers.19.self_att.layernorm_before_attention.weight      (1024,)             0.9663    0.2155    0.0407    0.8442    None      None     
 bert.encoder.layers.19.self_att.layernorm_before_attention.bias        (1024,)             0.8862    -0.2639   0.0446    -0.0175   None      None     
 bert.encoder.layers.19.self_att.self_attention.project_q.weight        (1024, 1024)        0.3286    -0.2766   0.0378    0.0000    None      None     
 bert.encoder.layers.19.self_att.self_attention.project_q.bias          (1024,)             0.5034    -0.4380   0.1097    -0.0034   None      None     
 bert.encoder.layers.19.self_att.self_attention.project_k.weight        (1024, 1024)        0.2964    -0.2935   0.0379    -0.0000   None      None     
 bert.encoder.layers.19.self_att.self_attention.project_k.bias          (1024,)             0.0187    -0.0218   0.0047    -0.0001   None      None     
 bert.encoder.layers.19.self_att.self_attention.project_v.weight        (1024, 1024)        0.1602    -0.1637   0.0300    -0.0000   None      None     
 bert.encoder.layers.19.self_att.self_attention.project_v.bias          (1024,)             0.0892    -0.1332   0.0163    -0.0002   None      None     
 bert.encoder.layers.19.self_att.self_attention.attention_out.weight    (1024, 1024)        0.1747    -0.1633   0.0287    0.0000    None      None     
 bert.encoder.layers.19.self_att.self_attention.attention_out.bias      (1024,)             0.1058    -0.1376   0.0316    0.0002    None      None     
 bert.encoder.layers.19.ffn.layernorm_before_ffn.weight                 (1024,)             3.0898    0.7329    0.1096    0.8184    None      None     
 bert.encoder.layers.19.ffn.layernorm_before_ffn.bias                   (1024,)             1.6562    -1.1816   0.1241    -0.0040   None      None     
 bert.encoder.layers.19.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.4141    -0.4644   0.0323    0.0000    None      None     
 bert.encoder.layers.19.ffn.ffn.w_in.w.bias                             (4096,)             0.5068    -0.2815   0.0246    -0.0677   None      None     
 bert.encoder.layers.19.ffn.ffn.w_out.weight                            (1024, 4096)        2.0918    -5.3711   0.0325    -0.0000   None      None     
 bert.encoder.layers.19.ffn.ffn.w_out.bias                              (1024,)             0.2664    -0.2756   0.0559    -0.0004   None      None     
 bert.encoder.layers.20.self_att.layernorm_before_attention.weight      (1024,)             0.9478    0.0869    0.0458    0.8325    None      None     
 bert.encoder.layers.20.self_att.layernorm_before_attention.bias        (1024,)             1.1406    -0.5728   0.0493    -0.0048   None      None     
 bert.encoder.layers.20.self_att.self_attention.project_q.weight        (1024, 1024)        0.4353    -0.3884   0.0375    0.0000    None      None     
 bert.encoder.layers.20.self_att.self_attention.project_q.bias          (1024,)             0.6714    -0.5981   0.1600    0.0060    None      None     
 bert.encoder.layers.20.self_att.self_attention.project_k.weight        (1024, 1024)        0.3621    -0.3730   0.0377    -0.0000   None      None     
 bert.encoder.layers.20.self_att.self_attention.project_k.bias          (1024,)             0.0238    -0.0211   0.0052    0.0001    None      None     
 bert.encoder.layers.20.self_att.self_attention.project_v.weight        (1024, 1024)        0.2220    -0.1946   0.0320    0.0000    None      None     
 bert.encoder.layers.20.self_att.self_attention.project_v.bias          (1024,)             0.0598    -0.0757   0.0151    -0.0004   None      None     
 bert.encoder.layers.20.self_att.self_attention.attention_out.weight    (1024, 1024)        0.1616    -0.2201   0.0303    -0.0000   None      None     
 bert.encoder.layers.20.self_att.self_attention.attention_out.bias      (1024,)             0.1069    -0.1473   0.0283    0.0003    None      None     
 bert.encoder.layers.20.ffn.layernorm_before_ffn.weight                 (1024,)             2.6191    0.7056    0.0867    0.7993    None      None     
 bert.encoder.layers.20.ffn.layernorm_before_ffn.bias                   (1024,)             1.7051    -0.9604   0.1028    0.0043    None      None     
 bert.encoder.layers.20.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.4695    -0.4785   0.0326    0.0000    None      None     
 bert.encoder.layers.20.ffn.ffn.w_in.w.bias                             (4096,)             0.6079    -0.1458   0.0255    -0.0622   None      None     
 bert.encoder.layers.20.ffn.ffn.w_out.weight                            (1024, 4096)        5.0195    -4.5156   0.0336    -0.0000   None      None     
 bert.encoder.layers.20.ffn.ffn.w_out.bias                              (1024,)             0.4854    -0.7168   0.0666    -0.0006   None      None     
 bert.encoder.layers.21.self_att.layernorm_before_attention.weight      (1024,)             0.9365    0.0468    0.0464    0.8657    None      None     
 bert.encoder.layers.21.self_att.layernorm_before_attention.bias        (1024,)             0.8306    -0.2734   0.0450    0.0033    None      None     
 bert.encoder.layers.21.self_att.self_attention.project_q.weight        (1024, 1024)        0.3188    -0.3179   0.0386    -0.0000   None      None     
 bert.encoder.layers.21.self_att.self_attention.project_q.bias          (1024,)             0.4163    -0.5991   0.1177    0.0019    None      None     
 bert.encoder.layers.21.self_att.self_attention.project_k.weight        (1024, 1024)        0.3325    -0.2297   0.0384    -0.0000   None      None     
 bert.encoder.layers.21.self_att.self_attention.project_k.bias          (1024,)             0.0199    -0.0207   0.0043    -0.0001   None      None     
 bert.encoder.layers.21.self_att.self_attention.project_v.weight        (1024, 1024)        0.2061    -0.2023   0.0312    0.0000    None      None     
 bert.encoder.layers.21.self_att.self_attention.project_v.bias          (1024,)             0.3074    -0.1813   0.0245    0.0012    None      None     
 bert.encoder.layers.21.self_att.self_attention.attention_out.weight    (1024, 1024)        0.1859    -0.1992   0.0308    -0.0000   None      None     
 bert.encoder.layers.21.self_att.self_attention.attention_out.bias      (1024,)             0.0833    -0.0811   0.0265    0.0001    None      None     
 bert.encoder.layers.21.ffn.layernorm_before_ffn.weight                 (1024,)             2.2324    0.7637    0.0654    0.8228    None      None     
 bert.encoder.layers.21.ffn.layernorm_before_ffn.bias                   (1024,)             1.4316    -0.4033   0.0816    0.0075    None      None     
 bert.encoder.layers.21.ffn.ffn.w_in.w.weight                           (4096, 1024)        1.0869    -1.4014   0.0331    0.0000    None      None     
 bert.encoder.layers.21.ffn.ffn.w_in.w.bias                             (4096,)             0.5957    -0.1300   0.0278    -0.0603   None      None     
 bert.encoder.layers.21.ffn.ffn.w_out.weight                            (1024, 4096)        5.6914    -4.3555   0.0333    -0.0000   None      None     
 bert.encoder.layers.21.ffn.ffn.w_out.bias                              (1024,)             0.6001    -0.5977   0.0723    -0.0004   None      None     
 bert.encoder.layers.22.self_att.layernorm_before_attention.weight      (1024,)             0.9658    0.0750    0.0482    0.8936    None      None     
 bert.encoder.layers.22.self_att.layernorm_before_attention.bias        (1024,)             0.8911    -0.2654   0.0448    0.0032    None      None     
 bert.encoder.layers.22.self_att.self_attention.project_q.weight        (1024, 1024)        0.3721    -0.3579   0.0393    -0.0000   None      None     
 bert.encoder.layers.22.self_att.self_attention.project_q.bias          (1024,)             0.4785    -0.4343   0.1064    0.0022    None      None     
 bert.encoder.layers.22.self_att.self_attention.project_k.weight        (1024, 1024)        0.2101    -0.2151   0.0390    0.0000    None      None     
 bert.encoder.layers.22.self_att.self_attention.project_k.bias          (1024,)             0.0173    -0.0211   0.0039    0.0001    None      None     
 bert.encoder.layers.22.self_att.self_attention.project_v.weight        (1024, 1024)        0.2169    -0.1946   0.0312    0.0000    None      None     
 bert.encoder.layers.22.self_att.self_attention.project_v.bias          (1024,)             0.0632    -0.0613   0.0149    -0.0002   None      None     
 bert.encoder.layers.22.self_att.self_attention.attention_out.weight    (1024, 1024)        0.2595    -0.2314   0.0313    0.0000    None      None     
 bert.encoder.layers.22.self_att.self_attention.attention_out.bias      (1024,)             0.0474    -0.0366   0.0119    0.0001    None      None     
 bert.encoder.layers.22.ffn.layernorm_before_ffn.weight                 (1024,)             1.5850    0.7944    0.0480    0.8340    None      None     
 bert.encoder.layers.22.ffn.layernorm_before_ffn.bias                   (1024,)             1.0527    -0.3118   0.0674    0.0081    None      None     
 bert.encoder.layers.22.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.8081    -0.9312   0.0334    0.0000    None      None     
 bert.encoder.layers.22.ffn.ffn.w_in.w.bias                             (4096,)             0.7388    -0.1914   0.0267    -0.0588   None      None     
 bert.encoder.layers.22.ffn.ffn.w_out.weight                            (1024, 4096)        6.9531    -6.5469   0.0337    0.0000    None      None     
 bert.encoder.layers.22.ffn.ffn.w_out.bias                              (1024,)             0.7324    -0.7017   0.0663    0.0001    None      None     
 bert.encoder.layers.23.self_att.layernorm_before_attention.weight      (1024,)             0.9985    0.0335    0.0533    0.9131    None      None     
 bert.encoder.layers.23.self_att.layernorm_before_attention.bias        (1024,)             0.8643    -0.1273   0.0393    0.0038    None      None     
 bert.encoder.layers.23.self_att.self_attention.project_q.weight        (1024, 1024)        0.2339    -0.2798   0.0405    0.0000    None      None     
 bert.encoder.layers.23.self_att.self_attention.project_q.bias          (1024,)             0.5303    -0.5234   0.1155    -0.0059   None      None     
 bert.encoder.layers.23.self_att.self_attention.project_k.weight        (1024, 1024)        0.2815    -0.3154   0.0399    -0.0000   None      None     
 bert.encoder.layers.23.self_att.self_attention.project_k.bias          (1024,)             0.0141    -0.0134   0.0036    0.0000    None      None     
 bert.encoder.layers.23.self_att.self_attention.project_v.weight        (1024, 1024)        0.2107    -0.2247   0.0331    0.0000    None      None     
 bert.encoder.layers.23.self_att.self_attention.project_v.bias          (1024,)             0.0535    -0.0411   0.0119    0.0009    None      None     
 bert.encoder.layers.23.self_att.self_attention.attention_out.weight    (1024, 1024)        0.3232    -0.1613   0.0332    -0.0000   None      None     
 bert.encoder.layers.23.self_att.self_attention.attention_out.bias      (1024,)             0.1810    -0.0339   0.0124    0.0001    None      None     
 bert.encoder.layers.23.ffn.layernorm_before_ffn.weight                 (1024,)             1.5742    0.7480    0.0433    0.8477    None      None     
 bert.encoder.layers.23.ffn.layernorm_before_ffn.bias                   (1024,)             1.5029    -0.1782   0.0789    0.0104    None      None     
 bert.encoder.layers.23.ffn.ffn.w_in.w.weight                           (4096, 1024)        0.3213    -0.3462   0.0341    -0.0000   None      None     
 bert.encoder.layers.23.ffn.ffn.w_in.w.bias                             (4096,)             0.1975    -0.3066   0.0253    -0.0555   None      None     
 bert.encoder.layers.23.ffn.ffn.w_out.weight                            (1024, 4096)        1.3008    -0.8311   0.0299    0.0000    None      None     
 bert.encoder.layers.23.ffn.ffn.w_out.bias                              (1024,)             0.1814    -0.0920   0.0338    0.0002    None      None     
 bert.encoder.output_layernorm.weight                                   (1024,)             0.8516    0.2325    0.0260    0.7192    None      None     
 bert.encoder.output_layernorm.bias                                     (1024,)             0.2040    -0.5684   0.0644    0.0076    None      None     
 bert.lm_head.dense.weight                                              (1024, 1024)        0.7256    -0.7598   0.0454    -0.0002   None      None     
 bert.lm_head.dense.bias                                                (1024,)             1.5537    -0.1301   0.0529    0.0156    None      None     
 bert.lm_head.layer_norm.weight                                         (1024,)             2.4902    0.0369    0.4456    1.9658    None      None     
 bert.lm_head.layer_norm.bias                                           (1024,)             0.8130    -1.4365   0.2097    0.0171    None      None     
 bert.lm_head.decoder.weight                                            (30522, 1024)       1.1377    -0.5918   0.0415    -0.0194   None      None     
 bert.lm_head.decoder.bias                                              (30522,)            1.3379    -6.1406   0.1224    -0.1281   None      None     
 bert.pooler.dense.weight                                               (1024, 1024)        0.2019    -0.2642   0.0301    -0.0000   None      None     
 bert.pooler.dense.bias                                                 (1024,)             0.0474    -0.0504   0.0154    -0.0000   None      None     
 dense.weight                                                           (2, 1024)           3.5234    -3.5273   1.0342    0.0020    None      None     
train | epoch   0 | Iter:      0/    50 | loss: 25.1249 | lr: 1.0000e-04, scale:   128.0000 | time: 2.531 avg_time: 2.531 | acc: 0.4167
train | epoch   0 | Iter:      1/    50 | loss: 24.2136 | lr: 1.0000e-04, scale:   128.0000 | time: 0.971 avg_time: 1.751 | acc: 0.5000
train | epoch   0 | Iter:      2/    50 | loss: 18.3156 | lr: 1.0000e-04, scale:   128.0000 | time: 2.135 avg_time: 1.879 | acc: 0.5833
train | epoch   0 | Iter:      3/    50 | loss: 12.9849 | lr: 1.0000e-04, scale:   128.0000 | time: 0.861 avg_time: 1.625 | acc: 0.6667
train | epoch   0 | Iter:      4/    50 | loss: 8.1752 | lr: 1.0000e-04, scale:   128.0000 | time: 0.900 avg_time: 1.480 | acc: 0.5833
train | epoch   0 | Iter:      5/    50 | loss: 1.9824 | lr: 1.0000e-04, scale:   128.0000 | time: 0.870 avg_time: 1.378 | acc: 0.2500
train | epoch   0 | Iter:      6/    50 | loss: 1.6584 | lr: 1.0000e-04, scale:   128.0000 | time: 0.859 avg_time: 1.304 | acc: 0.4167
train | epoch   0 | Iter:      7/    50 | loss: 4.5647 | lr: 1.0000e-04, scale:   128.0000 | time: 0.891 avg_time: 1.252 | acc: 0.6667
train | epoch   0 | Iter:      8/    50 | loss: 1.0610 | lr: 1.0000e-04, scale:   128.0000 | time: 0.891 avg_time: 1.212 | acc: 0.5833
train | epoch   0 | Iter:      9/    50 | loss: 3.9843 | lr: 1.0000e-04, scale:   128.0000 | time: 0.890 avg_time: 1.180 | acc: 0.5417
train | epoch   0 | Iter:     10/    50 | loss: 9.2914 | lr: 1.0000e-04, scale:   128.0000 | time: 0.857 avg_time: 1.150 | acc: 0.4583
train | epoch   0 | Iter:     11/    50 | loss: 1.6187 | lr: 1.0000e-04, scale:   128.0000 | time: 0.906 avg_time: 1.130 | acc: 0.5833
train | epoch   0 | Iter:     12/    50 | loss: 1.9761 | lr: 1.0000e-04, scale:   128.0000 | time: 0.859 avg_time: 1.109 | acc: 0.5833
train | epoch   0 | Iter:     13/    50 | loss: 1.5279 | lr: 1.0000e-04, scale:   128.0000 | time: 1.041 avg_time: 1.104 | acc: 0.7083
train | epoch   0 | Iter:     14/    50 | loss: 1.3229 | lr: 1.0000e-04, scale:   128.0000 | time: 0.871 avg_time: 1.089 | acc: 0.7083
train | epoch   0 | Iter:     15/    50 | loss: 1.2896 | lr: 1.0000e-04, scale:   128.0000 | time: 0.874 avg_time: 1.075 | acc: 0.5417
train | epoch   0 | Iter:     16/    50 | loss: 1.1241 | lr: 1.0000e-04, scale:   128.0000 | time: 0.872 avg_time: 1.063 | acc: 0.6667
train | epoch   0 | Iter:     17/    50 | loss: 1.1571 | lr: 1.0000e-04, scale:   128.0000 | time: 0.899 avg_time: 1.054 | acc: 0.4167
train | epoch   0 | Iter:     18/    50 | loss: 1.1262 | lr: 1.0000e-04, scale:   128.0000 | time: 0.867 avg_time: 1.045 | acc: 0.4167
train | epoch   0 | Iter:     19/    50 | loss: 1.0170 | lr: 1.0000e-04, scale:   128.0000 | time: 0.875 avg_time: 1.036 | acc: 0.4583
train | epoch   0 | Iter:     20/    50 | loss: 0.9225 | lr: 1.0000e-04, scale:   128.0000 | time: 0.888 avg_time: 1.029 | acc: 0.6667
train | epoch   0 | Iter:     21/    50 | loss: 0.8405 | lr: 1.0000e-04, scale:   128.0000 | time: 0.863 avg_time: 1.021 | acc: 0.5833
train | epoch   0 | Iter:     22/    50 | loss: 0.9243 | lr: 1.0000e-04, scale:   128.0000 | time: 0.885 avg_time: 1.015 | acc: 0.6250
train | epoch   0 | Iter:     23/    50 | loss: 1.0101 | lr: 1.0000e-04, scale:   128.0000 | time: 0.883 avg_time: 1.010 | acc: 0.5417
train | epoch   0 | Iter:     24/    50 | loss: 0.9721 | lr: 1.0000e-04, scale:   128.0000 | time: 0.915 avg_time: 1.006 | acc: 0.4583
train | epoch   0 | Iter:     25/    50 | loss: 0.9278 | lr: 1.0000e-04, scale:   128.0000 | time: 0.876 avg_time: 1.001 | acc: 0.4583
train | epoch   0 | Iter:     26/    50 | loss: 0.8623 | lr: 1.0000e-04, scale:   128.0000 | time: 0.874 avg_time: 0.996 | acc: 0.5417
train | epoch   0 | Iter:     27/    50 | loss: 1.0126 | lr: 1.0000e-04, scale:   128.0000 | time: 0.872 avg_time: 0.992 | acc: 0.4167
train | epoch   0 | Iter:     28/    50 | loss: 0.8301 | lr: 1.0000e-04, scale:   128.0000 | time: 0.925 avg_time: 0.990 | acc: 0.4583
train | epoch   0 | Iter:     29/    50 | loss: 0.8611 | lr: 1.0000e-04, scale:   128.0000 | time: 0.857 avg_time: 0.985 | acc: 0.5417
train | epoch   0 | Iter:     30/    50 | loss: 0.7849 | lr: 1.0000e-04, scale:   128.0000 | time: 0.902 avg_time: 0.983 | acc: 0.7500
train | epoch   0 | Iter:     31/    50 | loss: 0.7861 | lr: 1.0000e-04, scale:   128.0000 | time: 0.882 avg_time: 0.979 | acc: 0.5417
train | epoch   0 | Iter:     32/    50 | loss: 0.8642 | lr: 1.0000e-04, scale:   128.0000 | time: 0.908 avg_time: 0.977 | acc: 0.6250
train | epoch   0 | Iter:     33/    50 | loss: 0.7613 | lr: 1.0000e-04, scale:   128.0000 | time: 0.862 avg_time: 0.974 | acc: 0.5833
train | epoch   0 | Iter:     34/    50 | loss: 0.8208 | lr: 1.0000e-04, scale:   128.0000 | time: 0.888 avg_time: 0.971 | acc: 0.5417
train | epoch   0 | Iter:     35/    50 | loss: 0.8079 | lr: 1.0000e-04, scale:   128.0000 | time: 0.930 avg_time: 0.970 | acc: 0.7917
train | epoch   0 | Iter:     36/    50 | loss: 0.7290 | lr: 1.0000e-04, scale:   128.0000 | time: 0.876 avg_time: 0.968 | acc: 0.4583
train | epoch   0 | Iter:     37/    50 | loss: 0.8334 | lr: 1.0000e-04, scale:   128.0000 | time: 0.886 avg_time: 0.966 | acc: 0.6250
train | epoch   0 | Iter:     38/    50 | loss: 0.7612 | lr: 1.0000e-04, scale:   128.0000 | time: 0.885 avg_time: 0.964 | acc: 0.5417
train | epoch   0 | Iter:     39/    50 | loss: 0.7159 | lr: 1.0000e-04, scale:   128.0000 | time: 0.895 avg_time: 0.962 | acc: 0.7083
train | epoch   0 | Iter:     40/    50 | loss: 0.7722 | lr: 1.0000e-04, scale:   128.0000 | time: 0.878 avg_time: 0.960 | acc: 0.6250
train | epoch   0 | Iter:     41/    50 | loss: 0.6864 | lr: 1.0000e-04, scale:   128.0000 | time: 0.900 avg_time: 0.958 | acc: 0.5417
train | epoch   0 | Iter:     42/    50 | loss: 0.7049 | lr: 1.0000e-04, scale:   128.0000 | time: 0.898 avg_time: 0.957 | acc: 0.6667
train | epoch   0 | Iter:     43/    50 | loss: 0.8136 | lr: 1.0000e-04, scale:   128.0000 | time: 0.896 avg_time: 0.956 | acc: 0.4583
train | epoch   0 | Iter:     44/    50 | loss: 0.7754 | lr: 1.0000e-04, scale:   128.0000 | time: 0.909 avg_time: 0.955 | acc: 0.5833
train | epoch   0 | Iter:     45/    50 | loss: 0.7740 | lr: 1.0000e-04, scale:   128.0000 | time: 0.888 avg_time: 0.953 | acc: 0.4583
train | epoch   0 | Iter:     46/    50 | loss: 1.0359 | lr: 1.0000e-04, scale:   128.0000 | time: 0.888 avg_time: 0.952 | acc: 0.5833
train | epoch   0 | Iter:     47/    50 | loss: 0.8869 | lr: 1.0000e-04, scale:   128.0000 | time: 0.879 avg_time: 0.950 | acc: 0.5417
train | epoch   0 | Iter:     48/    50 | loss: 0.8491 | lr: 1.0000e-04, scale:   128.0000 | time: 0.879 avg_time: 0.949 | acc: 0.4167
train | epoch   0 | Iter:     49/    50 | loss: 0.9203 | lr: 1.0000e-04, scale:   128.0000 | time: 0.527 avg_time: 0.940 | acc: 0.0000
dev | epoch   0 | Iter:      0/    17 | loss: 1.1718
dev | epoch   0 | Iter:      1/    17 | loss: 1.5109
dev | epoch   0 | Iter:      2/    17 | loss: 0.9433
dev | epoch   0 | Iter:      3/    17 | loss: 1.4073
dev | epoch   0 | Iter:      4/    17 | loss: 0.8017
dev | epoch   0 | Iter:      5/    17 | loss: 1.2116
dev | epoch   0 | Iter:      6/    17 | loss: 1.2195
dev | epoch   0 | Iter:      7/    17 | loss: 1.4222
dev | epoch   0 | Iter:      8/    17 | loss: 1.4712
dev | epoch   0 | Iter:      9/    17 | loss: 1.5409
dev | epoch   0 | Iter:     10/    17 | loss: 1.0835
dev | epoch   0 | Iter:     11/    17 | loss: 0.6577
dev | epoch   0 | Iter:     12/    17 | loss: 1.3066
dev | epoch   0 | Iter:     13/    17 | loss: 0.9263
dev | epoch   0 | Iter:     14/    17 | loss: 1.3904
dev | epoch   0 | Iter:     15/    17 | loss: 1.2919
dev | epoch   0 | Iter:     16/    17 | loss: 1.0460
dev epoch 0:
accuracy: 38.24
train | epoch   1 | Iter:      0/    50 | loss: 1.3595 | lr: 1.0000e-04, scale:   128.0000 | time: 0.882 avg_time: 0.882 | acc: 0.4167
train | epoch   1 | Iter:      1/    50 | loss: 0.7493 | lr: 1.0000e-04, scale:   128.0000 | time: 1.044 avg_time: 0.963 | acc: 0.5000
train | epoch   1 | Iter:      2/    50 | loss: 1.1907 | lr: 1.0000e-04, scale:   128.0000 | time: 0.868 avg_time: 0.931 | acc: 0.5833
train | epoch   1 | Iter:      3/    50 | loss: 0.8634 | lr: 1.0000e-04, scale:   128.0000 | time: 0.986 avg_time: 0.945 | acc: 0.6250
train | epoch   1 | Iter:      4/    50 | loss: 0.8530 | lr: 1.0000e-04, scale:   128.0000 | time: 0.930 avg_time: 0.942 | acc: 0.4583
train | epoch   1 | Iter:      5/    50 | loss: 0.9700 | lr: 1.0000e-04, scale:   128.0000 | time: 0.905 avg_time: 0.936 | acc: 0.5833
train | epoch   1 | Iter:      6/    50 | loss: 0.8288 | lr: 1.0000e-04, scale:   128.0000 | time: 0.891 avg_time: 0.929 | acc: 0.5417
train | epoch   1 | Iter:      7/    50 | loss: 0.8249 | lr: 1.0000e-04, scale:   128.0000 | time: 0.864 avg_time: 0.921 | acc: 0.5833
train | epoch   1 | Iter:      8/    50 | loss: 0.7391 | lr: 1.0000e-04, scale:   128.0000 | time: 0.890 avg_time: 0.918 | acc: 0.5000
train | epoch   1 | Iter:      9/    50 | loss: 0.8422 | lr: 1.0000e-04, scale:   128.0000 | time: 0.904 avg_time: 0.916 | acc: 0.5833
train | epoch   1 | Iter:     10/    50 | loss: 0.7677 | lr: 1.0000e-04, scale:   128.0000 | time: 0.884 avg_time: 0.913 | acc: 0.5000
train | epoch   1 | Iter:     11/    50 | loss: 0.7932 | lr: 1.0000e-04, scale:   128.0000 | time: 0.889 avg_time: 0.911 | acc: 0.5833
train | epoch   1 | Iter:     12/    50 | loss: 0.8592 | lr: 1.0000e-04, scale:   128.0000 | time: 0.874 avg_time: 0.909 | acc: 0.5833
train | epoch   1 | Iter:     13/    50 | loss: 0.7329 | lr: 1.0000e-04, scale:   128.0000 | time: 0.897 avg_time: 0.908 | acc: 0.7500
train | epoch   1 | Iter:     14/    50 | loss: 0.8418 | lr: 1.0000e-04, scale:   128.0000 | time: 0.901 avg_time: 0.907 | acc: 0.3750
train | epoch   1 | Iter:     15/    50 | loss: 0.8004 | lr: 1.0000e-04, scale:   128.0000 | time: 0.889 avg_time: 0.906 | acc: 0.3750
train | epoch   1 | Iter:     16/    50 | loss: 0.7606 | lr: 1.0000e-04, scale:   128.0000 | time: 0.911 avg_time: 0.906 | acc: 0.6667
train | epoch   1 | Iter:     17/    50 | loss: 0.7705 | lr: 1.0000e-04, scale:   128.0000 | time: 0.885 avg_time: 0.905 | acc: 0.6250
train | epoch   1 | Iter:     18/    50 | loss: 0.7698 | lr: 1.0000e-04, scale:   128.0000 | time: 0.875 avg_time: 0.904 | acc: 0.5417
train | epoch   1 | Iter:     19/    50 | loss: 0.7667 | lr: 1.0000e-04, scale:   128.0000 | time: 0.872 avg_time: 0.902 | acc: 0.4167
train | epoch   1 | Iter:     20/    50 | loss: 0.7952 | lr: 1.0000e-04, scale:   128.0000 | time: 0.897 avg_time: 0.902 | acc: 0.5000
train | epoch   1 | Iter:     21/    50 | loss: 0.7571 | lr: 1.0000e-04, scale:   128.0000 | time: 0.907 avg_time: 0.902 | acc: 0.5417
train | epoch   1 | Iter:     22/    50 | loss: 0.8493 | lr: 1.0000e-04, scale:   128.0000 | time: 0.872 avg_time: 0.901 | acc: 0.4167
train | epoch   1 | Iter:     23/    50 | loss: 0.9494 | lr: 1.0000e-04, scale:   128.0000 | time: 0.880 avg_time: 0.900 | acc: 0.7083
train | epoch   1 | Iter:     24/    50 | loss: 0.8217 | lr: 1.0000e-04, scale:   128.0000 | time: 1.244 avg_time: 0.914 | acc: 0.4583
train | epoch   1 | Iter:     25/    50 | loss: 0.7966 | lr: 1.0000e-04, scale:   128.0000 | time: 0.904 avg_time: 0.913 | acc: 0.7500
train | epoch   1 | Iter:     26/    50 | loss: 0.7286 | lr: 1.0000e-04, scale:   128.0000 | time: 0.895 avg_time: 0.913 | acc: 0.4583
train | epoch   1 | Iter:     27/    50 | loss: 0.7845 | lr: 1.0000e-04, scale:   128.0000 | time: 0.920 avg_time: 0.913 | acc: 0.6667
train | epoch   1 | Iter:     28/    50 | loss: 0.7638 | lr: 1.0000e-04, scale:   128.0000 | time: 0.893 avg_time: 0.912 | acc: 0.3750
train | epoch   1 | Iter:     29/    50 | loss: 0.6986 | lr: 1.0000e-04, scale:   128.0000 | time: 0.868 avg_time: 0.911 | acc: 0.5833
train | epoch   1 | Iter:     30/    50 | loss: 0.7143 | lr: 1.0000e-04, scale:   128.0000 | time: 0.898 avg_time: 0.910 | acc: 0.7500
train | epoch   1 | Iter:     31/    50 | loss: 0.6876 | lr: 1.0000e-04, scale:   128.0000 | time: 0.876 avg_time: 0.909 | acc: 0.4583
train | epoch   1 | Iter:     32/    50 | loss: 0.7475 | lr: 1.0000e-04, scale:   128.0000 | time: 0.874 avg_time: 0.908 | acc: 0.5000
train | epoch   1 | Iter:     33/    50 | loss: 0.7314 | lr: 1.0000e-04, scale:   128.0000 | time: 0.910 avg_time: 0.908 | acc: 0.5417
train | epoch   1 | Iter:     34/    50 | loss: 0.7230 | lr: 1.0000e-04, scale:   128.0000 | time: 0.861 avg_time: 0.907 | acc: 0.6667
train | epoch   1 | Iter:     35/    50 | loss: 0.7723 | lr: 1.0000e-04, scale:   128.0000 | time: 0.933 avg_time: 0.908 | acc: 0.6667
train | epoch   1 | Iter:     36/    50 | loss: 0.7303 | lr: 1.0000e-04, scale:   128.0000 | time: 0.863 avg_time: 0.906 | acc: 0.5833
train | epoch   1 | Iter:     37/    50 | loss: 0.7755 | lr: 1.0000e-04, scale:   128.0000 | time: 0.902 avg_time: 0.906 | acc: 0.5833
train | epoch   1 | Iter:     38/    50 | loss: 0.7186 | lr: 1.0000e-04, scale:   128.0000 | time: 0.876 avg_time: 0.906 | acc: 0.6250
train | epoch   1 | Iter:     39/    50 | loss: 0.6704 | lr: 1.0000e-04, scale:   128.0000 | time: 0.876 avg_time: 0.905 | acc: 0.6250
train | epoch   1 | Iter:     40/    50 | loss: 0.8694 | lr: 1.0000e-04, scale:   128.0000 | time: 0.873 avg_time: 0.904 | acc: 0.7083
train | epoch   1 | Iter:     41/    50 | loss: 0.6995 | lr: 1.0000e-04, scale:   128.0000 | time: 0.902 avg_time: 0.904 | acc: 0.5000
train | epoch   1 | Iter:     42/    50 | loss: 0.7185 | lr: 1.0000e-04, scale:   128.0000 | time: 0.903 avg_time: 0.904 | acc: 0.6667
train | epoch   1 | Iter:     43/    50 | loss: 0.7430 | lr: 1.0000e-04, scale:   128.0000 | time: 2.291 avg_time: 0.935 | acc: 0.5833
train | epoch   1 | Iter:     44/    50 | loss: 0.7938 | lr: 1.0000e-04, scale:   128.0000 | time: 2.244 avg_time: 0.965 | acc: 0.6250
train | epoch   1 | Iter:     45/    50 | loss: 0.7530 | lr: 1.0000e-04, scale:   128.0000 | time: 0.914 avg_time: 0.963 | acc: 0.7500
train | epoch   1 | Iter:     46/    50 | loss: 0.7390 | lr: 1.0000e-04, scale:   128.0000 | time: 0.859 avg_time: 0.961 | acc: 0.5000
train | epoch   1 | Iter:     47/    50 | loss: 0.7290 | lr: 1.0000e-04, scale:   128.0000 | time: 0.912 avg_time: 0.960 | acc: 0.4583
train | epoch   1 | Iter:     48/    50 | loss: 0.8050 | lr: 1.0000e-04, scale:   128.0000 | time: 0.878 avg_time: 0.959 | acc: 0.7083
train | epoch   1 | Iter:     49/    50 | loss: 0.8723 | lr: 1.0000e-04, scale:   256.0000 | time: 0.357 avg_time: 0.946 | acc: 0.0000
dev | epoch   1 | Iter:      0/    17 | loss: 1.0464
dev | epoch   1 | Iter:      1/    17 | loss: 1.2506
dev | epoch   1 | Iter:      2/    17 | loss: 0.8383
dev | epoch   1 | Iter:      3/    17 | loss: 1.1974
dev | epoch   1 | Iter:      4/    17 | loss: 0.7240
dev | epoch   1 | Iter:      5/    17 | loss: 1.0475
dev | epoch   1 | Iter:      6/    17 | loss: 1.0418
dev | epoch   1 | Iter:      7/    17 | loss: 1.2216
dev | epoch   1 | Iter:      8/    17 | loss: 1.2528
dev | epoch   1 | Iter:      9/    17 | loss: 1.2631
dev | epoch   1 | Iter:     10/    17 | loss: 0.9437
dev | epoch   1 | Iter:     11/    17 | loss: 0.6133
dev | epoch   1 | Iter:     12/    17 | loss: 1.1112
dev | epoch   1 | Iter:     13/    17 | loss: 0.8325
dev | epoch   1 | Iter:     14/    17 | loss: 1.1941
dev | epoch   1 | Iter:     15/    17 | loss: 1.1047
dev | epoch   1 | Iter:     16/    17 | loss: 0.9328
dev epoch 1:
accuracy: 38.24
train | epoch   2 | Iter:      0/    50 | loss: 1.0442 | lr: 1.0000e-04, scale:   256.0000 | time: 0.874 avg_time: 0.874 | acc: 0.4167
train | epoch   2 | Iter:      1/    50 | loss: 0.7848 | lr: 1.0000e-04, scale:   256.0000 | time: 0.919 avg_time: 0.896 | acc: 0.3750
train | epoch   2 | Iter:      2/    50 | loss: 0.8949 | lr: 1.0000e-04, scale:   256.0000 | time: 0.899 avg_time: 0.897 | acc: 0.5417
train | epoch   2 | Iter:      3/    50 | loss: 0.7476 | lr: 1.0000e-04, scale:   256.0000 | time: 0.860 avg_time: 0.888 | acc: 0.6667
train | epoch   2 | Iter:      4/    50 | loss: 0.7259 | lr: 1.0000e-04, scale:   256.0000 | time: 0.887 avg_time: 0.888 | acc: 0.5417
train | epoch   2 | Iter:      5/    50 | loss: 0.8169 | lr: 1.0000e-04, scale:   256.0000 | time: 0.905 avg_time: 0.891 | acc: 0.3750
train | epoch   2 | Iter:      6/    50 | loss: 0.7434 | lr: 1.0000e-04, scale:   256.0000 | time: 0.864 avg_time: 0.887 | acc: 0.2917
train | epoch   2 | Iter:      7/    50 | loss: 0.7648 | lr: 1.0000e-04, scale:   256.0000 | time: 0.968 avg_time: 0.897 | acc: 0.7083
train | epoch   2 | Iter:      8/    50 | loss: 0.7893 | lr: 1.0000e-04, scale:   256.0000 | time: 0.873 avg_time: 0.894 | acc: 0.5000
train | epoch   2 | Iter:      9/    50 | loss: 0.6920 | lr: 1.0000e-04, scale:   256.0000 | time: 1.628 avg_time: 0.968 | acc: 0.4167
train | epoch   2 | Iter:     10/    50 | loss: 0.7210 | lr: 1.0000e-04, scale:   256.0000 | time: 0.878 avg_time: 0.960 | acc: 0.4167
train | epoch   2 | Iter:     11/    50 | loss: 0.7925 | lr: 1.0000e-04, scale:   256.0000 | time: 0.874 avg_time: 0.952 | acc: 0.5417
train | epoch   2 | Iter:     12/    50 | loss: 0.7156 | lr: 1.0000e-04, scale:   256.0000 | time: 0.895 avg_time: 0.948 | acc: 0.5417
train | epoch   2 | Iter:     13/    50 | loss: 0.7003 | lr: 1.0000e-04, scale:   256.0000 | time: 0.902 avg_time: 0.945 | acc: 0.7500
train | epoch   2 | Iter:     14/    50 | loss: 0.6758 | lr: 1.0000e-04, scale:   256.0000 | time: 0.898 avg_time: 0.942 | acc: 0.5833
train | epoch   2 | Iter:     15/    50 | loss: 0.6932 | lr: 1.0000e-04, scale:   256.0000 | time: 0.881 avg_time: 0.938 | acc: 0.6250
train | epoch   2 | Iter:     16/    50 | loss: 0.7878 | lr: 1.0000e-04, scale:   256.0000 | time: 0.883 avg_time: 0.935 | acc: 0.5417
train | epoch   2 | Iter:     17/    50 | loss: 0.7445 | lr: 1.0000e-04, scale:   256.0000 | time: 0.876 avg_time: 0.931 | acc: 0.6250
train | epoch   2 | Iter:     18/    50 | loss: 0.6486 | lr: 1.0000e-04, scale:   256.0000 | time: 0.879 avg_time: 0.929 | acc: 0.7083
train | epoch   2 | Iter:     19/    50 | loss: 0.7545 | lr: 1.0000e-04, scale:   256.0000 | time: 0.875 avg_time: 0.926 | acc: 0.4167
train | epoch   2 | Iter:     20/    50 | loss: 0.7696 | lr: 1.0000e-04, scale:   256.0000 | time: 0.878 avg_time: 0.924 | acc: 0.4167
train | epoch   2 | Iter:     21/    50 | loss: 0.7499 | lr: 1.0000e-04, scale:   256.0000 | time: 0.876 avg_time: 0.921 | acc: 0.4167
train | epoch   2 | Iter:     22/    50 | loss: 0.7652 | lr: 1.0000e-04, scale:   256.0000 | time: 0.878 avg_time: 0.920 | acc: 0.5833
train | epoch   2 | Iter:     23/    50 | loss: 0.8618 | lr: 1.0000e-04, scale:   256.0000 | time: 1.011 avg_time: 0.923 | acc: 0.6667
train | epoch   2 | Iter:     24/    50 | loss: 0.7827 | lr: 1.0000e-04, scale:   256.0000 | time: 0.919 avg_time: 0.923 | acc: 0.4583
train | epoch   2 | Iter:     25/    50 | loss: 0.7688 | lr: 1.0000e-04, scale:   256.0000 | time: 0.870 avg_time: 0.921 | acc: 0.5417
slurmstepd: error: *** JOB 159866 ON g0022 CANCELLED AT 2022-03-20T23:27:42 ***
